{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络\n",
    "class lstm_model(nn.Module):\n",
    "    def __init__(self, vocab, hidden_size, num_layers, dropout=0.5):\n",
    "        super(lstm_model, self).__init__()\n",
    "        self.vocab = vocab \n",
    "        self.int_char = {i : char for i, char in enumerate(vocab)} \n",
    "        self.char_int = {char : i for i, char in self.int_char.items()}\n",
    "        self.encoder = OneHotEncoder(sparse=False).fit(vocab.reshape(-1, 1)) \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        # lstm层\n",
    "        self.lstm = nn.LSTM(len(vocab), hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.linear = nn.Linear(hidden_size, len(vocab)) # 这里的输出shape是每个字符的得分\n",
    "        \n",
    "    def forward(self, sequence, hs=None):\n",
    "        out, hs = self.lstm(sequence, hs) # lstm的输出格式：（batch_size, sequence_length, hidden_size）\n",
    "        out = out.reshape(-1, self.hidden_size) # 这里需要将out转换为linear的输入格式，即(batch_size*sequence_length, hidden_size)\n",
    "        output = self.linear(out) # linear的输出格式：((batch_size*sequence_length, vocab_size)\n",
    "        return output, hs\n",
    "        \n",
    "    def onehot_encode(self, data):\n",
    "        return self.encoder.transform(data)\n",
    "    \n",
    "    def onehot_decode(self, data):\n",
    "        return self.encoder.inverse_transform(data)\n",
    "    \n",
    "    def label_encode(self, data):\n",
    "        return np.array([self.char_int[ch] for ch in data])\n",
    "    \n",
    "    def label_decode(self, data):\n",
    "        return np.array([self.int_char[ch] for ch in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义构建新数据集的批处理方法\n",
    "def get_batches(data, batch_size, seq_len):\n",
    "\n",
    "    num_features = data.shape[1] \n",
    "    num_chars = batch_size * seq_len \n",
    "    num_batches = int(np.floor(len(data) / num_chars)) \n",
    "    need_chars = num_batches * num_chars\n",
    "    targets = np.append(data[1:], data[0]).reshape(data.shape) \n",
    "    inputs = data[:need_chars] \n",
    "    targets = targets[:need_chars] \n",
    "    \n",
    "    # shape转换\n",
    "    inputs = inputs.reshape(batch_size, -1, num_features)\n",
    "    targets = targets.reshape(batch_size, -1, num_features)\n",
    "    \n",
    "    # 构建新的数据集\n",
    "    for i in range(0, inputs.shape[1], seq_len):\n",
    "        x = inputs[:, i : i+seq_len]\n",
    "        y = targets[:, i : i+seq_len]\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, data, batch_size, seq_len, epochs, lr=0.01, valid=None):\n",
    "\n",
    "    # 是否有cuda\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if valid is not None:\n",
    "        data = model.onehot_encode(data.reshape(-1, 1))\n",
    "        valid = model.onehot_encode(valid.reshape(-1, 1))\n",
    "    else:\n",
    "        data = model.onehot_encode(data.reshape(-1, 1))\n",
    "    # 保存损失值\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # 循环训练（验证）\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        hs = None # hs 等于 hidden_size,隐藏层结点\n",
    "        train_ls = 0.0\n",
    "        val_ls = 0.0\n",
    "        for x, y in get_batches(data, batch_size, seq_len):\n",
    "             # 梯度置零\n",
    "            optimizer.zero_grad()\n",
    "            x = torch.tensor(x).float().to(device) # 类型转换\n",
    "            # 模型训练\n",
    "            out, hs = model(x, hs)\n",
    "            hs = ([h.data for h in hs]) \n",
    "            y = y.reshape(-1, len(model.vocab))\n",
    "            y = model.onehot_decode(y)\n",
    "            y = model.label_encode(y.squeeze())\n",
    "            y = torch.from_numpy(y).long().to(device)\n",
    "            loss = criterion(out, y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_ls += loss.item()\n",
    "        \n",
    "        if valid is not None:\n",
    "            # 开始验证\n",
    "            model.eval()\n",
    "            hs = None\n",
    "            with torch.no_grad():\n",
    "                for x, y in get_batches(valid, batch_size, seq_len):\n",
    "                    x = torch.tensor(x).float().to(device)\n",
    "                    out, hs = model(x, hs) # 预测输出\n",
    "                    hs = ([h.data for h in hs])\n",
    "                    \n",
    "                    y = y.reshape(-1, len(model.vocab))\n",
    "                    y = model.onehot_decode(y)\n",
    "                    y = model.label_encode(y.squeeze())\n",
    "                    y = torch.from_numpy(y).long().to(device)\n",
    "                    \n",
    "                    loss = criterion(out, y.squeeze())\n",
    "                    val_ls += loss.item()\n",
    "                    \n",
    "                val_loss.append(np.mean(val_ls)) # 求出每一轮的损失均值，并累计\n",
    "                \n",
    "            train_loss.append(np.mean(train_ls)) # 求出每一轮的损失均值，并累计\n",
    "            \n",
    "        print(f'--------------Epochs{epochs} | {epoch}---------------')\n",
    "        print(f'Train Loss : {train_loss[-1]}') # 这里-1为最后添加进去的loss值，即本轮batch的loss\n",
    "        if val_loss:\n",
    "            print(f'Val Loss : {val_loss[-1]}')\n",
    "            \n",
    "    # 绘制loss曲线\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def throw_trash(string: str) -> str:\n",
    "    reg: re.Pattern = re.compile('^[0-9a-zA-Z_]{1,}$')\n",
    "    string: str = reg.sub(' ', string)\n",
    "    string = re.sub( ' +',' ', string)\n",
    "    string = string.replace('\\n','')\n",
    "    string=string.replace(\"\\'\",\"'\")\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter 1happy families are all alike; every unhappy family is unhappy in its ownway.everything was '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\downloads\\\\anna.txt\") as data:\n",
    "    text = data.read()\n",
    "text=throw_trash(text)\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.',\n",
       "       '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';',\n",
       "       '?', '@', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z'], dtype='<U1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(sorted(set(text)))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符的数量\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len = int(np.floor(0.2 * len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555604,)\n",
      "(388901,)\n"
     ]
    }
   ],
   "source": [
    "trainset = np.array(list(text[:-val_len]))\n",
    "validset = np.array(list(text[-val_len:]))\n",
    "print(trainset.shape)\n",
    "print(validset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (lstm): LSTM(56, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=512, out_features=56, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "batch_size = 128\n",
    "seq_len = 100\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "model = lstm_model(vocab, hidden_size, num_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epochs50 | 0---------------\n",
      "Train Loss : 362.3254086971283\n",
      "Val Loss : 84.53495693206787\n",
      "--------------Epochs50 | 1---------------\n",
      "Train Loss : 320.5443913936615\n",
      "Val Loss : 72.57386445999146\n",
      "--------------Epochs50 | 2---------------\n",
      "Train Loss : 276.7701494693756\n",
      "Val Loss : 63.997851610183716\n",
      "--------------Epochs50 | 3---------------\n",
      "Train Loss : 246.4478178024292\n",
      "Val Loss : 56.10816693305969\n",
      "--------------Epochs50 | 4---------------\n",
      "Train Loss : 220.44631016254425\n",
      "Val Loss : 51.02923321723938\n",
      "--------------Epochs50 | 5---------------\n",
      "Train Loss : 203.19574356079102\n",
      "Val Loss : 47.72372257709503\n",
      "--------------Epochs50 | 6---------------\n",
      "Train Loss : 191.03313851356506\n",
      "Val Loss : 45.1748366355896\n",
      "--------------Epochs50 | 7---------------\n",
      "Train Loss : 181.56514525413513\n",
      "Val Loss : 43.388057589530945\n",
      "--------------Epochs50 | 8---------------\n",
      "Train Loss : 174.6627584695816\n",
      "Val Loss : 42.18847990036011\n",
      "--------------Epochs50 | 9---------------\n",
      "Train Loss : 169.4619710445404\n",
      "Val Loss : 41.31118905544281\n",
      "--------------Epochs50 | 10---------------\n",
      "Train Loss : 165.33527171611786\n",
      "Val Loss : 40.86647927761078\n",
      "--------------Epochs50 | 11---------------\n",
      "Train Loss : 162.1534172296524\n",
      "Val Loss : 40.07397902011871\n",
      "--------------Epochs50 | 12---------------\n",
      "Train Loss : 159.45136177539825\n",
      "Val Loss : 39.64446294307709\n",
      "--------------Epochs50 | 13---------------\n",
      "Train Loss : 156.91068720817566\n",
      "Val Loss : 39.3485369682312\n",
      "--------------Epochs50 | 14---------------\n",
      "Train Loss : 155.0624017715454\n",
      "Val Loss : 39.049431681632996\n",
      "--------------Epochs50 | 15---------------\n",
      "Train Loss : 153.44350600242615\n",
      "Val Loss : 38.78897738456726\n"
     ]
    }
   ],
   "source": [
    "train(model, trainset, batch_size, seq_len, epochs, lr=lr, valid=validset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char, top_k = None, hidden_size = None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        char = np.array([char]) \n",
    "        char = char.reshape(-1, 1) \n",
    "        char_encoding = model.onehot_encode(char) \n",
    "        char_encoding = char_encoding.reshape(1, 1, -1) \n",
    "        char_tensor = torch.tensor(char_encoding, dtype=torch.float32) \n",
    "        char_tensor = char_tensor.to(device) \n",
    "        out, hidden_size = model(char_tensor, hidden_size) \n",
    "        probs = F.softmax(out, dim=1).squeeze() \n",
    "\n",
    "        if top_k is None:\n",
    "            indices = np.arange(vocab_size)\n",
    "        else:\n",
    "            probs, indices = probs.topk(top_k) # 选取概率最大的前top_k个\n",
    "            indices = indices.cpu().numpy()\n",
    "        \n",
    "        probs = probs.cpu().numpy()\n",
    "        \n",
    "        char_index = np.random.choice(indices, p = probs / probs.sum()) # 随机选取一个索引\n",
    "        char = model.int_char[char_index] # 获取索引对应的字符\n",
    "        \n",
    "    return char, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取一个样本\n",
    "def sample(model, length, top_k = None, sentence=\"every unhappy family \"):\n",
    "    hidden_size = None # 初始化\n",
    "    new_sentence = [char for char in sentence] # 初始化\n",
    "    for i in range(length):\n",
    "        next_char, hidden_size = predict(model, new_sentence[-1], top_k = top_k, hidden_size = hidden_size) # 预测下一个字符\n",
    "        new_sentence.append(next_char)\n",
    "        \n",
    "    return ''.join(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = sample(model, 2000, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'every unhappy family mieiehthed to hte hem mind to hess anded, hast serteng, indes and inded hy thet and hy the sas inter tt andeshing..\"thans, sateds how hisse to hesing hom hit has, sond her the thised then ard at an hind, sout tas ander hamilg his as tagd and this he woutt is, shout thoner the sint asditilg hond thanens hered has to handsing tor hid tom she went hus i fit shet hes hed ang tan tilid the thes he med ald tinding hed it han sound tin hime hads he her had the sit if ind sas hes, ander him to mas sor henders anded, hid hed i matthingsinged hud tom thensen his site sere at ham, ind the ming as the gadled. sestileds tad the paddsen sased he wamed, has tomligtinged he mat tas ang tissend serer hes he he sand ind and her, as thithing,s he sinds, a gase hed if sas ill the song hid ind an she mime ham thing has,,.\"ing illering sis ind, hes shethed indesse that her silling hom sishine ham sin hat the sises sithen hid his sering has, in he sis herese whate her hem mong, the hemp tomes his he sett a sel tas i sas the shit t thinged, ind, siting himid hadd,\"sens sall and hened ind tout has, shim. thamengen sev huming, haming to a sount tome hed thes in thit tout in a sas tham had an sheme hersind the sesth, ind itteng, shite wited, a thith tar sering,\"sess and sand tiseringes he hishis as tis ittilg. an th shas hang,s he mede ant soud ander sotser sad al whathered he witin an hem,.\"\"thing shics sor sas sas has, hed toudd anded in she menses and iner, as his istilg andantest had inded. the sag her anged seving. ish thas her hands, soulg hiss in sagded,.\"herse tale iseteng hamid,\"\"\"th the hesing sit her hes hed sher itshed hed he sis hoshe hom his and i fads he wishe and ind heren sithed thenen sed ather,\" sheng sis at serd ind, hing, in thang tome sitherilg thalger, he the till sessinde tomer hes,,\"\"sishs sit som sand, has, arers at she song, i she mintind the man souts, ase sis and he sadsidses ind sese wamang,\"ssines hos has a mat hime wots shoud al tore houge and has he mem ins ha'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lstm_model.net\"\n",
    "\n",
    "checkpoint = {\n",
    "    'hidden_size' : model.hidden_size,\n",
    "    'num_layers' : model.num_layers,\n",
    "    'state_dict' : model.state_dict()\n",
    "}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
